\documentclass[a4paper,12pt]{article}

\usepackage[utf8]{inputenc}   
\usepackage[dutch]{babel}     
\usepackage{amsmath, amssymb} 

\usepackage{graphicx}         
\usepackage[hidelinks]{hyperref}         
\usepackage{geometry}         
\usepackage{float}            
\usepackage{parskip}          

\geometry{a4paper, margin=2.5cm}

\title{\textbf{Edge AI Portfolio}}
\author{Yves Braeckman}
\date{\today}

\begin{document}

% -------------------------------------------------------------------
% TITELPAGINA
% -------------------------------------------------------------------
\maketitle
\thispagestyle{empty} % Geen paginanummer op de titelpagina
\newpage

% -------------------------------------------------------------------
% INHOUDSTAFEL
% -------------------------------------------------------------------
\tableofcontents
\newpage

% -------------------------------------------------------------------
% DEEL 1: ALGEMEEN & LABO'S
% -------------------------------------------------------------------
\section{Inleiding}
% Hier beschrijf je kort de context van dit portfolio.
Dit document vormt het portfolio voor het opleidingsonderdeel Edge AI.

De wereld van AI evolueert razendsnel. Waar de focus vroeger vaak lag op zware cloud-berekeningen, zien we nu een verschuiving naar lokale intelligentie. Om deze technologie effectief toe te passen, is echter meer nodig dan enkel het kunnen schrijven van code. Een diepgaand inzicht in de volledige data-pipeline, van het verzamelen en opschonen van ruwe data tot het trainen, evalueren en deployen van modellen, is essentieel.

Dit portfolio is opgebouwd uit twee grote delen die deze vaardigheden weerspiegelen:

\begin{enumerate} \item \textbf{Deel 1: Lessen en Labo's}: In dit deel reflecteer ik op de theoretische hoofdstukken en de uitgevoerde labo-opdrachten. Hierbij ligt de nadruk niet op het louter opsommen van definities, maar op mijn persoonlijke inzichten. Ik beschrijf hoe mijn begrip van concepten als data-exploratie, manipulatie en model-training is veranderd en welke valkuilen ik heb leren herkennen.

\item \textbf{Deel 2: Het Project}: Het tweede deel focust op de praktische integratie van deze kennis in het groepsproject. Hierin hebben we een afvalsorteersysteem ontwikkeld met behulp van Computer Vision op een Google Coral Dev Board. Dit deel demonstreert de vertaalslag van theorie naar een werkend prototype.
\end{enumerate}

Met dit verslag wil ik aantonen dat ik niet alleen de technische vaardigheden beheers om AI-modellen te bouwen, maar ook de kritische mindset heb ontwikkeld die nodig is om data correct te interpreteren en verantwoorde modellen te ontwikkelen.


\newpage
\section{Portfolio: Lessen en Labo's}
\subsection{Leren uit data}
Bij het doornemen van dit hoofdstuk werd me eigenlijk pas echt duidelijk hoe groot het verschil is tussen hoe wij als mensen leren en hoe we dat proberen na te bootsen in machines. Ik vond het heel interresant om te zien hoe een ML model eigenlijk een wiskundige definitie is van een functie die de data zo goed mogelijk probeert te benaderen.

Ik dacht vroeger dat een AI gewoon weet wat iets is, maar het hoofdstuk over de MNIST-dataset liet goed zien dat het eigenlijk draait om waarschijnlijkheden. Het model zegt niet dit is een 6, maar ik ben 99,9 zeker dat dit een 6 is. 

Tot slot is het onderscheid tussen ML en AI voor mij nu duidelijker. ML is het herkennen van het patroon, AI is de stap erna: er daadwerkelijk een autonome actie aan koppelen. 

Ik heb geleerd dat machine learning eigenlijk gewoon wiskunde is die probeert de werkelijkheid te imiteren, zonder dat het model echt 'begrijpt' wat het ziet. Het belangrijkste inzicht dat ik meeneem is dat we altijd rekening moeten houden met onzekerheid omdat het altijd een benadering van de realiteit blijft. Nu ik ook het verschil tussen het model en de toepassing scherp heb, voel ik me beter voorbereid om met de praktijkopdrachten aan de slag te gaan.

\subsection{Data}
De term "Garbage In, Garbage Out" kende ik al wel, maar dit hoofdstuk heeft die term voor mij veel concreter gemaakt. Ik dacht eerst dat het vooral ging over 'foute' getallen, maar ik begrijp nu dat het probleem eerder is: begrijp ik eigenlijk wel waar ik naar kijk en begrijp ik waarover ik verwacht dat het ML model een voorspelling maakt?

Een echte eye-opener was voor mij de sectie over het vinden van externe data. Ik had er niet bij stil gestaan dat een goede dataset vinden echt een uitdaging op zich is. Het idee dat je kritisch moet zijn over de bron, de meetmethode en zelfs de context van de data is iets wat ik zeker ga meenemen in mijn project verder in het semester. 

De uitleg over datatypes vond ik best complex. Voorheen zag ik data gewoon als text of getal. Het onderscheid tussen ordinaal, nominaal en intervalschalen is was nieuw voor mij. Vooral het stukje over circulaire data vond ik interresant: je kunt inderdaad niet zomaar het gemiddelde nemen van 23:00 uur en 01:00 uur, want dan kom je wiskundig op de middag uit, terwijl het middernacht moet zijn. 

Ik heb ook het belang van het codeboek ingezien. Zonder dat document kan je de cijfers in een dataset vaak niet correct interpreteren.

Mijn belangrijkste inzicht uit dit hoofdstuk is dat de kwaliteit van mijn AI-model al bepaald wordt ver voordat ik de eerste regel code schrijf. Het verzamelen, begrijpen en opschonen van data is misschien wel de belangrijkste stap. Ik heb geleerd dat ik kritisch moet kijken naar databronnen, rekening moet houden met de specifieke eigenschappen van variabelen en altijd op zoek moet gaan naar het codeboek om de context van de cijfers te begrijpen. Zonder die basis is het niet verantwoord om verder te gaan.

\subsection{Data exploratie}
Data exploratie leek me vooraf vooral snel wat grafiekjes maken om te zien hoe de data eruit ziet. Dit hoofdstuk liet me zien dat het eigenlijk de belangrijkste fase is om fouten in een later stadium te voorkomen. De DIPKEVER-checklist vond ik een handig ezelsbruggetje. Het voorbeeld van de terrorisme-dataset liet zien hoe makkelijk je misleid kan worden: als een jaartal als een float wordt ingelezen in plaats van een int, kan dat later voor rare bugs zorgen.

Het concept dat de meeste indruk maakte, was de log-transformatie bij de behandelingsduur. In de eerste grafiek zag de data er enorm scheef uit, maar door de natuurlijke logaritme te nemen, veranderde dit in een normaalverdeling. Dit liet me zien dat data soms 'verstopt' zit en dat je wiskundige technieken nodig hebt om het bruikbaar te maken voor statistische modellen. 

Exploratory Data Analysis is voor mij verschoven van grafieken maken naar detectivewerk. Ik heb geleerd dat ik eerst de structuur moet controleren voordat ik inhoudelijk ga analyseren. Mijn belangrijkste inzicht is dat visualisatie niet alleen dient om data te presenteren, maar vooral om dataproblemen zoals scheve verdelingen of outliers op te sporen die mijn model kunnen verpesten. Ik ga voortaan zeker de DIPKEVER-regels toepassen en kritischer kijken naar hoe data verdeeld is voordat ik verder ga.

\subsection{Manipuleren van data}
Wat ik vooral interessant vond in dit hoofdstuk, is de nuance bij het slicen van de dataset. Het selecteren van variabelen (kolommen) doen we continu, maar het filteren van instanties (rijen) is iets waar we voorzichtig mee moeten zijn. In de theorie werd benadrukt dat je rijen niet zomaar mag weggooien, tenzij er een gegronde reden is, zoals duidelijke invoerfouten of privacykwesties.

Ik heb geleerd dat transformaties soms noodzakelijk zijn om systemen met elkaar te laten praten. Simpele dingen zoals het aanpassen van encodings, het trimmen van spaties in tekstvariabelen of het toevoegen van voorloopnullen lijken simpel, maar zijn cruciaal om data te standaardiseren. Zonder deze stap kunnen ogenschijnlijk gelijke ID's door de computer als verschillend worden gezien, wat analyses fout laat lopen.

Tot slot hebben we gekeken naar het herschalen van numerieke waarden. Vooral normalisatie en standaardisatiezijn technieken die ik zeker ga toepassen in toekomstige opdrachten, omdat veel Edge AI-algoritmes slechter presteren als de schaal van variabelen te ver uit elkaar ligt.

Conclusie In dit onderdeel heb ik geleerd hoe ik ruwe data kan omzetten naar een bruikbare dataset voor analyse. Ik heb in de praktijk gebracht hoe ik uitschieters kan identificeren en verwijderen met behulp van visualisaties zoals de density plot en booleaanse maskers. Mijn belangrijkste resultaat is het inzicht dat elke manipulatie, of het nu gaat om het weglaten van een rij of het afronden van een getal, informatieverlies kan betekenen. Het is dus mijn taak om deze keuzes transparant te rapporteren en te verantwoorden, zodat de analyse integer blijft.


\subsection{Trainen en testen}
Het belangrijkste inzicht uit dit hoofdstuk was voor mij de realisatie dat Machine Learning geen exacte wetenschap of toverkunst is waarbij we altijd het gewenste resultaat bekomen. We moeten leren leven met onzekerheid en onze verwachtingen bijstellen van "we zullen voorspellen" naar "we hopen te kunnen voorspellen". 

Ik heb geleerd dat strengheid hierbij cruciaal is. De test-set moet volledig verborgen blijven voor het algoritme en eigenlijk ook voor mij tot het allerlaatste moment. Het was leerzaam om te zien dat er naast de training- en test-set vaak nog een validatie-set nodig is. 

Een interessant detail vond ik de uitzondering voor tijdreeksen. Waar we normaal data willekeurig door elkaar gooien om een goede split te krijgen, mag dit absoluut niet bij data met een tijdscomponent vanwege autocorrelatie. Hier moeten we echt een knip in de tijd maken, bijvoorbeeld alles na een bepaald jaartal gebruiken als test.

De workflow voor deep learning gaf mij veel inzicht in het concept overfitting. Door te kijken naar de loss van zowel de training- als de validatie-set tijdens de epochs, kun je precies zien wanneer het model stopt met leren en begint met 'uit het hoofd leren'. 

Mijn belangrijkste conclusie is dat een model pas waarde heeft als het zich heeft bewezen op data die het nog nooit heeft gezien en dat ik als ontwikkelaar altijd waakzaam moet zijn voor valkuilen zoals data lekkage die mijn resultaten kunnen vertekenen

\subsection{Labo 1}
Ik heb in dit labo geleerd hoe ik een dataset kan inladen en verkennen met behulp van Python en Pandas. Het was interessant om te zien hoe ik met slechts een paar regels code al een goed beeld kon krijgen van de structuur en inhoud van de data.

Ik heb geleerd hoe ik missende waarden kan identificeren en hoe ik basisstatistieken kan berekenen om inzicht te krijgen in de distributie van de variabelen.

Daarnaast heb ik geleerd hoe ik visualisaties kan maken met Matplotlib om patronen en relaties in de data te ontdekken. Door een correlatiematrix en heatmap te maken, kreeg ik inzicht in welke variabelen daadwerkelijk invloed hadden op de te voorspellen variabele. Dit bevestigde voor mij het nut van stap 2 uit de opdracht: zonder deze visuele check tast je eigenlijk in het duister over welke features relevant zijn.

Mijn belangrijkste inzicht is dat een grondige verkenning van de data essentieel is voordat ik verder ga met analyse of modellering. Het helpt me om potentiële problemen te identificeren en beter geïnformeerde beslissingen te nemen over hoe ik de data verder wil verwerken. Dit is ook nog eens extra duidelijk geworden tijdens het Hack The Future event, waar ik een ML opdracht had.
Hier heb ik de vaardigheden die ik in dit labo heb geleerd kunnen toepassen om de dataset te verkennen en voor te bereiden voor modellering.

\newpage
\subsection{Labo 2}
Ik heb in dit labo geleerd hoe ik bewerkingen op matrices kan doen in Python met behulp van de TensorFlow library. vervolgens heb ik ook geleerd hoe ik verschillende soorten data in TensorFlow kan krijgen.
Het laatste deel van het labo heeft mij voor de eerste keer een duidelijk beeld gegeven van hoe we te werk gaan voor het opstellen van een ML model.
Dit heeft mij geholpen om ook de Theorie beter te begrijpen omdat dit deel van het labo de theorie tastbaar maakte. 

\subsection{Labo 3}
Ik heb in dit labo geleerd hoe een belagrijke rol ML speelt in het dagelijkse leven. Het was heel interessant om te zien hoe google hun verzamelde data en ML modellen gebruikt om ons gepersonaliseerde advertenties te tonen en aankopen te voorspellen.
Ik vond het in het begin lastig om te werken met de google Big Query omgeving, maar na een tijdje had ik de basis onder de knie. 

Ik heb geleerd hoe ik SQL queries kan schrijven om data op te halen en te analyseren. Het was interessant om te zien hoe ik met behulp van SQL verschillende inzichten kon verkrijgen uit de grote datasets die google ter beschikking stelt.


\subsection{Labo 4}

\subsubsection{Beschrijving en Aanpak}
\textbf{Aanpak:}
Tijdens het maken van het labo heb ik volgende stappen doorlopen: 

\begin{enumerate}
    \item \textbf{Data Acquisitie en Preprocessing:} 
    Als eerste stap heb ik scaling toegepast op de dataset zodat alle afbeeldingen van uint8 [0, 255] naar float32 [0.0, 1.0] werden omgezet.    
    
    \item \textbf{Training en testen:} 
    De tweede stap was het verdelen van de data in een traning stuk en een test stuk om op het einde de performantie te kunnen testen.

    \item \textbf{Model bouwen:} 
    Na een beetje opzoekwerk had ik gelezen dat een Conv2D laag beter werkt voor afbeeldingen dan een Dense laag. Ik heb vervolgens verschillende combinaties van lagen geprobeerd en verschillende groottes van lagen.
    
    \item \textbf{Training en Compilatie:} 
    Het model is gecompileerd met de \texttt{Adam} optimizer. Ik heb hier gekozen voor een heel lage learning rate omdat het model anders stopte met leren. Het model is getraind gedurende 15 epochs.
\end{enumerate}

\subsubsection{Persoonlijke Inzichten en Reflectie}
% BELANGRIJK VOOR PUNTEN:
% - Beschrijf je inzichten in de context van de theorie.
% - Wat liep er mis? Hoe heb je dit opgelost?
% - Geen samenvatting van de theorie, maar JOUW perspectief.
Ik vond dit labo interresant omdat dit alles van de vorige labo's een beetje samenbracht. Ik vond het wel lastig om het model te bouwen omdat we hier niet veel over gezien hebben.
Dit was dus een trail-en-error process en liep niet vlot. 

Ik heb uiteindelijk online opgezocht en gebruik gemaakt van LLM's om te begrijpen wat alle parameters doen bij het bouwen van een model.
Zo heb ik bijgeleerd over de learning rate en het belang van dit getal bij de training. Wanneer ik dit te hoog koos, ging het model in volledige chaos en leerde het niet, maar als ik het laag koos bleef het model steken op een bepaalde accuracy.


\newpage

% -------------------------------------------------------------------
% DEEL 2: HET PROJECT (INDIVIDUEEL DEEL)
% -------------------------------------------------------------------
\section{Edge AI Project: Rapportage}
% INSTRUCTIE: Dit vervangt de aparte projectdocumentatie. Dit deel is INDIVIDUEEL.

\subsection{Projectintroductie}
% EISEN: Maximaal 1 pagina.
% 1. Motivatie (Waarom dit onderwerp?)
% 2. Doel (Wat wil je bereiken?)
% 3. Aanpak (Tools, technologie, opbouw)
% 4. Opzet van het systeem (Globale werking en structuur)
% TIP: Mag overlappen met input van de groep, maar schrijf in EIGEN woorden.

\subsubsection*{Motivatie}
Voor het project van Edge AI, heb ik, samen met mijn groep, gekozen om een afvalsorteersysteem te maken op basis van machine learning.
Ik vond dit onderwerp interresant omdat dit een brug bouwt tussen sustainable IT en Edge AI, we gebruiken IT om duurzamer te zijn. 
Ook vond ik het mooi een daglijks probleem te kunnen aanpakken, aangezien er vaak fouten tegen gemaakt worden en het veel efficienter zou kunnen wanneer dit automatisch gebeurt.

\subsubsection*{Doel}
Het doel van dit project is het ontwerpen en trainen van een machine learning model dat in staat is om vijf verschillende afvalcategorieën correct te classificeren. 
Daarnaast is het doel om dit model te implementeren op een Coral Dev Board. 
Hiermee willen we aantonen dat het systeem in staat is om op basis van visuele input een fysieke actie aan te sturen. 
Binnen dit project simuleren we de aansturing van een sorteermachine visueel met behulp van vijf LED's, waarbij elke LED correspondeert met een specifieke afvalstroom.

\subsubsection*{Aanpak}
Voor de praktische uitwerking van het project is er gebruik gemaakt van Roboflow voor het uitbreiden en cleanen van de dataset, Python met het TensorFlow library voor het ontwerpen en trainen van het model en een Coral Dev Board met een webcamera.

\subsubsection*{Opzet van het systeem}
De technische structuur van het systeem is opgebouwd als een eindeloze lus in Python die op het Coral Dev Board draait.

\begin{enumerate}
    \item \textbf{Initialisatie:} 
    Bij het opstarten laadt het script het geconverteerde .tflite-model in het geheugen van de Edge TPU en worden de GPIO-pinnen voor de vijf LED's geconfigureerd als output.
    
    \item \textbf{Input} 
    De lus begint met het ophalen van een frame van de live webcam-feed. Dit beeld wordt voorbewerkt door het te schalen naar de input-resolutie die het model verwacht.
    
    \item \textbf{Inferentie} 
    De Edge TPU voert de berekeningen uit op het beeld. Het resultaat is een lijst met waarschijnlijkheidsscores voor elk van de vijf afvalcategorieën.
    
    \item \textbf{Output:} 
    De code controleert of de hoogste score boven een vooraf ingestelde drempelwaarde ligt. Als een van de score's boven de drempel ligt, dan wordt via de GPIO-interface de LED aangestuurd die overeenkomt met de herkende categorie. Indien niets met zekerheid wordt herkend, blijven de LED's uit.
\end{enumerate}

Dit proces herhaalt zich heel snel, wat zorgt voor een real-time reactie wanneer een gebruiker afval voor de camera houdt.


\subsection{Eigen Bijdrage en Leerproces}
% EISEN:
% 1. Wat heb jij PERSOONLIJK bijgedragen? (Taken, code, testen, documentatie).
% 2. Welke keuzes heb jij beïnvloed en waarom?
% 3. Wat heb je geleerd? (Technisch én persoonlijk).
% 4. Eventuele moeilijkheden of inzichten.

\subsubsection{Wat heb ik bijgedragen?}

Mijn bijdrage aan het Edge AI-project was vooral in 3 onderdelen: data, documentatie en hardware-implementatie.

Ten eerste was ik verantwoordelijk voor de voorbereiding van de dataset. Ik heb hiervoor de keuze gemaakt om Roboflow in te zetten. Dit stelde mij in staat om efficiënt data-augmentaties toe te passen en direct de benodigde splits (train/validatie/test) te genereren. 

Ten tweede heb ik de technische implementatie op het device verder verzorgd. Na dat Sander het model op de hardware had doen werken, heb ik de Python-code geschreven voor de GPIO-aansturing en de labeling op het scherm.

Tot slot heb ik in de Project Guide de hoofdstukken 'Hardware' en 'Opstartprocedure' geschreven.

\subsubsection{Wat heb ik geleerd en welke uitdagingen waren er?}
Ik heb de volledige workflow van een Machine Learning-project leren beheersen: van data zoeken tot deployment op een edge device. Op persoonlijk vlak heeft dit project mijn zelfvertrouwen vergroot in het werken met embedded systemen voor ML. 

Een aanzienlijke uitdaging was de integratie van het model op de Coral Dev Board. Dit verliep initieel stroef, waardoor ik mij heb moeten verdiepen in de documentatie en specifieke libraries van Coral/Google om de GPIO aansturing werkend te krijgen.

Daarnaast vormde de output-verwerking een obstakel: het model gaf enkel numerieke ID's terug. Ik heb logica moeten schrijven om deze ID's real-time te mappen naar leesbare klassenamen op het scherm. 

\newpage

% -------------------------------------------------------------------
% BIJLAGEN / APPENDIX (GROEPSWERK)
% -------------------------------------------------------------------
\appendix
\section{Appendix: Technical Project Guide}
% INSTRUCTIE: Dit deel is identiek voor alle groepsleden.
% Dit is de "Handleiding voor wie het project opnieuw wil opstarten".

\subsection{Benodigde Hardware}
\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.5} 
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Hardware Component} & \textbf{Specificatie / Opmerking} \\ 
        \hline
        Google Coral Dev Board & Om code op te draaien \\ 
        \hline
        USB-C Kabel & Voor stroomtoevoer\\ 
        \hline
        Ethernet Kabel & Voor SSH \\ 
        \hline
        USB Webcam & Input voor beeldherkenning \\ 
        \hline
        5x LED & Rood, Groen, Geel, Blauw, Wit \\ 
        \hline
        5x Weerstand & $330\Omega$ \\ 
        \hline
        Breadboard & Voor het prototype circuit \\ 
        \hline
        Jumper Wires & Male-to-Male / Male-to-Female \\ 
        \hline
        PC / Laptop & Voor SSH-toegang en opstarten \\ 
        \hline
    \end{tabular}
    \caption{Lijst van benodigde hardware}
\end{table}

\subsection{Software en Dependencies}
% Bv. Python packages, TensorFlow, EdgeTPU libraries...

\subsection{Installatie-instructies}
% Hoe installeer je de drivers, runtime, NVIDA tools, AI-omgeving?

\subsection{Opstartprocedure}
\subsubsection*{1. Hardware Opstelling}
Voordat het bord wordt ingeschakeld, moeten alle componenten correct verbonden zijn:

\begin{enumerate}
    \item \textbf{Camera:} Sluit de USB-webcam aan op de USB-A poort van het Coral Dev Board. Richt de camera op een witte achtergrond met voldoende belichting.
    \item \textbf{Circuit:} Sluit de LED's en weerstanden aan op het breadboard en verbind deze met de GPIO-pinnen volgens het bedradingsschema.
    \item \textbf{Netwerk:} Verbind de ethernetkabel met het bord en laptop voor SSH.
    \item \textbf{Stroom:} Sluit als laatste de USB-C voedingskabel aan op de poort met het label PWR.
\end{enumerate}

\subsubsection*{2. Verbinding en Uitvoering}
\textbf{Stap A: Verbind via SSH} \\
Open een terminal en maak verbinding met het bord:
\begin{verbatim}
ssh mendel@192.168.1.100
\end{verbatim}

\textbf{Stap B: Navigeer naar de projectmap} \\
Ga naar de folder waar de broncode en het model staan:
\begin{verbatim}
cd /home/mendel/Edge-AI-Project
\end{verbatim}

\textbf{Stap C: Start de applicatie} \\
Voer het Python-script uit om de detectie te starten.
\begin{verbatim}
python3 main.py
\end{verbatim}

\textbf{Stap D: De applicatie gebruiken} \\
Ga op de laptop/PC naar de browser en surf naar 192.168.1.100:5000

\subsubsection*{3. Stoppen van het systeem}
Om het programma te stoppen, druk je in de terminal op:
\begin{verbatim}
CTRL + C
\end{verbatim}

\subsection{Testinstructies en Model Performantie}
% CRITIAAL ONDERDEEL:
% 1. Hoe test je in de praktijk? (Hardware check, dependencies check).
% 2. Hoe activeer je het model? (Commando's, verwachte uitkomsten).
% 3. Demo-data scenario's.

\subsubsection{Model Analyse}
% - Welk ML-model (CNN, MobileNet, ...)?
% - Performantie op testdata (Accuracy, Precision/Recall, Confusion Matrix).
% - Bron van de scores (Training logs vs echte demo).
% - Drempelwaardes (Thresholds) en motivatie.

\subsubsection{Foutafhandeling en Beperkingen}
% - Wat gebeurt er bij foute voorspellingen?
% - Realisme op edge device (latency, stabiliteit).
% - Vergelijking met alternatieven?
% - Bekende problemen.

\end{document}